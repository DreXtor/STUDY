{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom keras.layers import Dense,Dropout,GlobalAveragePooling2D,BatchNormalization\nfrom  keras import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator,img_to_array,load_img\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\n%matplotlib inline\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Context**\n\nThis is image data of Natural Scenes around the world.\n\n**Content**\n\nThis Data contains around 25k images of size 150x150 distributed under 6 categories.\n{'buildings' -> 0,\n'forest' -> 1,\n'glacier' -> 2,\n'mountain' -> 3,\n'sea' -> 4,\n'street' -> 5 }\n\nThe Train, Test and Prediction data is separated in each zip files. There are around 14k images in Train, 3k in Test and 7k in Prediction.\nThis data was initially published on https://datahack.analyticsvidhya.com by Intel to host a Image classification Challenge.\n\n**Acknowledgements**\n\nThanks to https://datahack.analyticsvidhya.com for the challenge and Intel for the Data\n\nPhoto by Jan Böttinger on Unsplash\n\n**Inspiration**\n\nWant to build powerful Neural network that can classify these images with more accuracy."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# path 입력\ntrain_dir = '../input/intel-image-classification/seg_train/seg_train/'\ntest_dir = '../input/intel-image-classification/seg_test/seg_test/'\npred_dir = '../input/intel-image-classification/seg_pred/seg_pred/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#디렉토리 내부 살펴보기\nos.listdir(train_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"album = pd.DataFrame(columns = [\"id\",\"label\"])\ndirs = ['mountain', 'street', 'buildings', 'sea', 'forest', 'glacier']\n# 각 디렉토리당 사진 정렬\nfor direc in dirs:\n    #print(direc)\n    for file in os.listdir(train_dir + direc):\n        album = album.append({\"id\":file,\"label\":direc},ignore_index = True)\nalbum.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#각 항목의 데이터의 수\nalbum['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_album = pd.DataFrame(columns = [\"id\",\"label\"])\ndirs = ['mountain', 'street', 'buildings', 'sea', 'forest', 'glacier']\n# 테스트에 들어있는 사진 정렬\nfor direc in dirs:\n    for file in os.listdir(test_dir + direc):\n        test_album = test_album.append({\"id\":file,\"label\":direc},ignore_index = True)\ntest_album.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train디고렉토리 내부 각 항목별 갯수 이미지화해서 확인\nsns.countplot(x=\"label\", data=album)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"label\", data=test_album)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,16))\nn = 0\n# 항목들 이미지 보기\nfor label in dirs:\n    n += 1\n    # .where(a,b) : Replace values where the condition is False\n    # a에는 조건을 넣어주고 그 조건이 안되는 값은 Nan값이 되거나 b로 값을 바꿈(b생략가능)\n    # img는 (albun['id']에서 album['label']값이 lavel과 같지 않으면 Nan값이 됨 -> 난값은 dropna()으로 없어짐)의 0번째\n    # 각 항목별로 하나씩만 가져오기 위해서\n    img = (album[\"id\"].where(album[\"label\"] == label).dropna()).iloc[0]\n    # 3열 2행으로 각자 1~6번째 자리로 순서대로 들어가게 된다\n    plt.subplot(3,2,n)\n    plt.subplots_adjust(hspace = 0.4,wspace = 0.2)\n    path = train_dir + label + '/' + img\n    plt.imshow(Image.open(path))\n    # 파일의 가로,세로,채널 정보 기입\n    plt.xlabel((img_to_array(Image.open(path))).shape)\n    plt.title(label)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate(train_dir,test_dir):\n    # train data data augmentation\n    train_datagen = ImageDataGenerator(rescale=1./255,horizontal_flip=True,validation_split = 0.2)\n    # test data에는 augmentation을 하면 안된다\n    test_datagen = ImageDataGenerator(rescale=1./255)\n    \n    # train data 설정\n    train_gen = train_datagen.flow_from_directory(\n      directory = train_dir,\n      subset=\"training\",\n      target_size = (150,150),\n      shuffle = True,\n      class_mode = 'categorical',\n      batch_size=500)\n    \n    # validation data 설정\n    val_gen = train_datagen.flow_from_directory(\n      directory = train_dir,\n      subset=\"validation\",\n      shuffle = True,\n      class_mode = 'categorical',\n      target_size=(150,150),\n      batch_size=500)\n    \n    # test data 설정\n    test_gen = test_datagen.flow_from_directory(\n      directory = test_dir,\n      shuffle = True,\n      class_mode = 'categorical',\n      target_size = (150,150),\n      batch_size = 500)\n    \n    return train_gen,val_gen,test_gen","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen,valid_gen,test_gen = generate(train_dir,test_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trans learning\nfrom keras.applications import DenseNet121","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델을 어떻게 불러올 것인가\ndef get_model(arch):\n    base_model = arch(include_top = False,\n                        input_shape = (150,150,3),\n                        weights = \"imagenet\")\n    base_model.trainable = False\n    print(\"Length of Model: \",len(list((base_model.layers))))\n    return base_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 그래프를 보여주는 함수\ndef plot_history(history):\n    # summarize history for accuracy\n    plt.plot(history.history['categorical_accuracy'])\n    plt.plot(history.history['val_categorical_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('categorical_accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('categorical_accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 레이어 구성하는 함수\ndef hypertune(train_gen,valid_gen,test_gen,arch = DenseNet121,lr = 0.005,m =128,n =512):    \n    base_model = get_model(arch)\n    gal = GlobalAveragePooling2D()(base_model.output)\n    d1 = Dense(units = m,activation = 'relu')(gal)\n    drop1 = Dropout(0.2)(d1)\n    d2 = Dense(units = n,activation = 'relu')(drop1)\n    drop2 = Dropout(0.4)(d2)\n    bn = BatchNormalization()(drop2)\n    output = Dense(units = 6,activation = \"softmax\")(bn)\n    model = tf.keras.Model(inputs = base_model.input,outputs = output)\n    model.compile(optimizer = tf.keras.optimizers.Adam(lr = lr),loss = \"categorical_crossentropy\",metrics = [\"categorical_accuracy\"])\n    history = model.fit(train_gen,validation_data = valid_gen,batch_size = 500,steps_per_epoch = 11230 // 500,epochs = 5)\n    loss,acc = model.evaluate(test_gen)\n    return [model,history]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values = hypertune(train_gen,valid_gen,test_gen)\nmodel = values[0]\nhistory = values[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = list(train_gen.class_indices)\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = img_to_array(Image.open('../input/intel-image-classification/seg_train/seg_train/street/1000.jpg'))\nx = x/255\nx = np.expand_dims(x,axis = 0)\nx.shape\nprint(classes[np.argmax(model.predict(x))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_album = pd.DataFrame(columns = [\"id\",\"label\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(pred_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_album.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for file in os.listdir(pred_dir):\n    # 파일의 path설정\n    path = pred_dir + file\n    # img를 배열로 바꿔주고\n    x = img_to_array(Image.open(path))\n    x = x/255\n    # np.expand_dims() : 차원을 추가\n    x = np.expand_dims(x,axis = 0)\n    # np.argmax() : 최댓값의 인덱스\n    pred = classes[np.argmax(model.predict(x))]\n    pred_album = pred_album.append({\"id\":file,\"label\":pred},ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_album.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"showcase = pred_album.iloc[:16,:]\nplt.figure(figsize = (20,16))\nn = 0\n# zip() : 동일한 개수로 이루어진 자료형을 묶어 주는 역할을 하는 함수\nfor image,label in zip(list(showcase[\"id\"]),list(showcase[\"label\"])):\n    n += 1\n    plt.subplot(4,4,n)\n    plt.subplots_adjust(hspace= 0.4,wspace = 0.4)\n    path = pred_dir + image\n    plt.imshow(Image.open(path))\n    plt.title('{}'.format(label))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# csv파일로 내보냄\npred_album.to_csv('answers.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for label in dirs:\n    # 클래스별 첫번째 사진만 \n    file = (album[\"id\"].where(album[\"label\"] == label).dropna()).iloc[0]\n    path = train_dir + label + '/' + file\n    sample_img = load_img(path, target_size=(150,150,3))\n    plt.imshow(sample_img)\n    sample_img = np.expand_dims(sample_img, axis=0)\n    sample_img = tf.keras.applications.densenet.preprocess_input(sample_img)\n    extractor = tf.keras.Model(inputs = model.input,outputs = model.layers[4].output)\n    feature_maps = extractor.predict(sample_img)\n    plt.figure(figsize = (20,16))\n    n = 0\n    for i in range(32):\n        n += 1\n        plt.subplot(8,4,n)\n        plt.subplots_adjust()\n        # plot filter channel in grayscale\n        plt.imshow(feature_maps[0, :, :, n-1], cmap='gray')\n        # show the figure\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}